{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"QA-Shlyahin-submit-Parts-1-2-3.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Проект QA\n","\n","## Yes/No Questions\n","\n","Вы будете работать с корпусом BoolQ. Корпус состоит из вопросов, предполагающих бинарный ответ (да / нет), абзацев из Википедии,  содержащих ответ на вопрос, заголовка статьи, из которой извлечен абзац и непосредственно ответа (true / false).\n","\n","Корпус описан в статье:\n","\n","Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n","BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n","\n","https://arxiv.org/abs/1905.10044\n","\n","\n","Корпус (train-dev split) доступен в репозитории проекта:  https://github.com/google-research-datasets/boolean-questions\n","\n","Используйте для обучения train часть корпуса, для валидации и тестирования – dev часть. \n","\n","Каждый бонус пункт оцениватся в 1 балл. Вывод являяется обязательным!"],"metadata":{"id":"Fd06vTCzqj3y"}},{"cell_type":"code","source":["!gsutil cp gs://boolq/train.jsonl .\n","!gsutil cp gs://boolq/dev.jsonl ./"],"metadata":{"id":"FOup-Ca-q5Ay","executionInfo":{"status":"ok","timestamp":1644700770321,"user_tz":-180,"elapsed":4560,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"c8ec096d-ad69-4dfb-9a63-e1c22db1c409","execution":{"iopub.status.busy":"2022-02-13T06:51:01.753209Z","iopub.execute_input":"2022-02-13T06:51:01.753526Z","iopub.status.idle":"2022-02-13T06:51:09.088664Z","shell.execute_reply.started":"2022-02-13T06:51:01.753495Z","shell.execute_reply":"2022-02-13T06:51:09.087579Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Copying gs://boolq/train.jsonl...\n/ [1 files][  6.2 MiB/  6.2 MiB]                                                \nOperation completed over 1 objects/6.2 MiB.                                      \nCopying gs://boolq/dev.jsonl...\n/ [1 files][  2.1 MiB/  2.1 MiB]                                                \nOperation completed over 1 objects/2.1 MiB.                                      \n","output_type":"stream"}]},{"cell_type":"code","source":["train_path = \"./train.jsonl\"\n","dev_path = \"./dev.jsonl\""],"metadata":{"id":"k5hixjuOzM3w","execution":{"iopub.status.busy":"2022-02-13T06:51:09.092524Z","iopub.execute_input":"2022-02-13T06:51:09.092807Z","iopub.status.idle":"2022-02-13T06:51:09.099251Z","shell.execute_reply.started":"2022-02-13T06:51:09.092776Z","shell.execute_reply":"2022-02-13T06:51:09.098224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"AkgNpO5vkXVd","executionInfo":{"status":"ok","timestamp":1644700782618,"user_tz":-180,"elapsed":12300,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"b7f1570a-1d68-4fc2-e205-f7c976f088b9","execution":{"iopub.status.busy":"2022-02-13T06:51:09.101482Z","iopub.execute_input":"2022-02-13T06:51:09.101904Z","iopub.status.idle":"2022-02-13T06:51:19.621408Z","shell.execute_reply.started":"2022-02-13T06:51:09.101859Z","shell.execute_reply":"2022-02-13T06:51:19.620216Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":["import random\n","import torch\n","from torch import nn\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n","# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import transformers as ppb\n","\n","import re\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.notebook import tqdm\n","\n","from torch.utils.data import Dataset, random_split, Subset, DataLoader, Sampler\n","\n","import torch.optim as optim\n","\n","import time\n","import math\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","%matplotlib inline\n","from IPython.display import clear_output\n","\n","import gc\n","\n","import json\n","import os\n","import nltk\n","import torch\n","\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","from torchtext.vocab import GloVe\n","\n","\n","from nltk.tokenize.stanford import StanfordTokenizer\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","from dataclasses import dataclass, field"],"metadata":{"id":"_fizTGuBsaPr","execution":{"iopub.status.busy":"2022-02-13T06:51:19.624079Z","iopub.execute_input":"2022-02-13T06:51:19.624878Z","iopub.status.idle":"2022-02-13T06:51:19.645473Z","shell.execute_reply.started":"2022-02-13T06:51:19.624831Z","shell.execute_reply":"2022-02-13T06:51:19.643789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# зафиксируем рандомы для воспроизводимости результатов\n","\n","random.seed(26)\n","np.random.seed(26)\n","torch.manual_seed(26)"],"metadata":{"id":"x4TUDFtUypbn","executionInfo":{"status":"ok","timestamp":1644700791892,"user_tz":-180,"elapsed":7,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"de8a4bdd-29f0-477c-ea5d-19122a933b8f","execution":{"iopub.status.busy":"2022-02-13T06:51:19.649237Z","iopub.execute_input":"2022-02-13T06:51:19.650058Z","iopub.status.idle":"2022-02-13T06:51:19.666404Z","shell.execute_reply.started":"2022-02-13T06:51:19.650013Z","shell.execute_reply":"2022-02-13T06:51:19.664944Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f2100225550>"},"metadata":{}}]},{"cell_type":"code","source":["train_data_df = pd.read_json(\"train.jsonl\", lines=True, orient='records')\n","dev_data_df = pd.read_json(\"dev.jsonl\", lines=True, orient=\"records\")\n","\n","passages_train = train_data_df.passage.values\n","questions_train = train_data_df.question.values\n","answers_train = train_data_df.answer.values.astype(int)\n","\n","passages_dev = dev_data_df.passage.values\n","questions_dev = dev_data_df.question.values\n","answers_dev = dev_data_df.answer.values.astype(int)"],"metadata":{"id":"gZ1V6E5xyv71","execution":{"iopub.status.busy":"2022-02-13T06:51:19.668761Z","iopub.execute_input":"2022-02-13T06:51:19.669458Z","iopub.status.idle":"2022-02-13T06:51:19.819508Z","shell.execute_reply.started":"2022-02-13T06:51:19.669350Z","shell.execute_reply":"2022-02-13T06:51:19.818342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["questions_dev[-5:]"],"metadata":{"id":"3PHT3C2Kzm_H","executionInfo":{"status":"ok","timestamp":1644700817880,"user_tz":-180,"elapsed":252,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"075b7e96-f8b3-41a5-8298-83680ed682c1","execution":{"iopub.status.busy":"2022-02-13T06:51:19.821320Z","iopub.execute_input":"2022-02-13T06:51:19.821616Z","iopub.status.idle":"2022-02-13T06:51:19.828905Z","shell.execute_reply.started":"2022-02-13T06:51:19.821572Z","shell.execute_reply":"2022-02-13T06:51:19.827719Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array(['is manic depression the same as bi polar',\n       'was whiskey galore based on a true story',\n       'are there plants on the international space station',\n       'does the hockey puck have to cross the line to be a goal',\n       'will there be a season 5 of shadowhunters'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":["Для проверки моделей приготовим тестовыe примеры."],"metadata":{"id":"3BtvqDEDzTMp","execution":{"iopub.status.busy":"2022-02-13T06:50:09.600872Z","iopub.execute_input":"2022-02-13T06:50:09.605279Z","iopub.status.idle":"2022-02-13T06:50:09.621662Z","shell.execute_reply.started":"2022-02-13T06:50:09.605239Z","shell.execute_reply":"2022-02-13T06:50:09.611245Z"}}},{"cell_type":"code","source":["test_passages = passages_dev[-5:]\n","test_questions = questions_dev[-5:]\n","test_answers = answers_dev[-5:]"],"metadata":{"id":"WWI6XZ96zWV5","execution":{"iopub.status.busy":"2022-02-13T06:51:19.830824Z","iopub.execute_input":"2022-02-13T06:51:19.831580Z","iopub.status.idle":"2022-02-13T06:51:19.841192Z","shell.execute_reply.started":"2022-02-13T06:51:19.831500Z","shell.execute_reply":"2022-02-13T06:51:19.840214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Пример вопроса: \n","question: is batman and robin a sequel to batman forever\n","\n","title: Batman & Robin (film)\n","\n","answer: true\n","\n","passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."],"metadata":{"id":"yDEsCCkoqj34"}},{"cell_type":"markdown","source":["# Соображения по методологии\n","\n","# Проведение одного эксперимента\n","1. Подбирайте гиперпараметры! Это довольно важно. Гиперпараметры --- это размеры слоёв, например. Хотя бы в одном эксперименте это стоит сделать честно, в остальных экспериментах в принципе можно предполагать, что в предыдущем эксперименте оптимальные размеры модели уже найдены.\n","2. Часто вам придётся скачивать предобученные веса (вроде BERT или RoBERTa) и дообучать дополнительные веса. В таком случае стоит попробовать несколько разных подходов:\n","* обучать только новые (головные) слои \n","* обучать совместно всю модель\n","* сначала обучить головные слои, потом дофайнтьюнить веса всей модели\n","3. Фиксируйте лучшие результаты на валидационной выборке, по валидационной же выборке останавливайте обучение\n","4. Графики, графики, графики\n","5. Делайте промежуточные выводы\n","\n","# Общие соображения\n","1. Один эксперимент --- одно изменение! Вы не представляете, насколько это важно. Пример: для эксперимента с моделью 1 Петя для получения эмбеддингов скачал предобученные веса BERT, а для эксперимента с моделью 2 -- предобученные веса RoBERTa. Вторая модель оказалась лучше, и Петя подумал, что архитектура второй модели лучше. Но, оказывается, с весами RoBERTa модель 1 работала бы не менее хорошо. \n","2. Всегда сравнивайте результаты эксперимента с результатами всех остальных экспериментов и делайте промежуточные выводы. ***Это самый важный пункт!***\n","3. Не забывайте протестировать модель на адекватность. Для этого нужно задавать ей реальные вопросы и смотреть на ответы. Вопросы не должны быть элементами обучающей выборки!\n"],"metadata":{"id":"5G6nqak9RiWA"}},{"cell_type":"markdown","source":["## Часть 1. Эксплоративный анализ (1 балл)\n","1. Посчитайте долю yes и no классов в корпусе\n","2. Оцените среднюю длину вопроса\n","3. Оцените среднюю длину параграфа\n","4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "],"metadata":{"id":"Iu5CnuYdqj3_"}},{"cell_type":"code","source":["passages_corpus = np.hstack([passages_train, passages_dev])\n","questions_corpus = np.hstack([questions_train, questions_dev])\n","answers_corpus = np.hstack([answers_train, answers_dev])"],"metadata":{"id":"z4c0fZv20_48","execution":{"iopub.status.busy":"2022-02-13T06:51:19.842525Z","iopub.execute_input":"2022-02-13T06:51:19.844962Z","iopub.status.idle":"2022-02-13T06:51:19.856868Z","shell.execute_reply.started":"2022-02-13T06:51:19.844932Z","shell.execute_reply":"2022-02-13T06:51:19.855719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noyes = np.bincount(answers_corpus)\n","print(\"NO - \", noyes[0]/noyes.sum(), \", YES - \", noyes[1]/noyes.sum()  )"],"metadata":{"id":"vWcU0WkS1A6L","executionInfo":{"status":"ok","timestamp":1641821662120,"user_tz":-180,"elapsed":270,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"3c212777-cb0d-471e-8f5c-e5f89142d005","execution":{"iopub.status.busy":"2022-02-13T06:51:19.858910Z","iopub.execute_input":"2022-02-13T06:51:19.859767Z","iopub.status.idle":"2022-02-13T06:51:19.869731Z","shell.execute_reply.started":"2022-02-13T06:51:19.859721Z","shell.execute_reply":"2022-02-13T06:51:19.868315Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"NO -  0.3772544695597385 , YES -  0.6227455304402615\n","output_type":"stream"}]},{"cell_type":"code","source":["lens_question = np.array([len(x.split()) for x in questions_corpus])\n","print('mean length of question ', lens_question.mean())"],"metadata":{"id":"53pDJDZtRWGZ","executionInfo":{"status":"ok","timestamp":1641823188187,"user_tz":-180,"elapsed":364,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"24b2301d-3419-4b5c-9f93-5572fb5ea19c","execution":{"iopub.status.busy":"2022-02-13T06:51:19.871197Z","iopub.execute_input":"2022-02-13T06:51:19.873660Z","iopub.status.idle":"2022-02-13T06:51:19.892814Z","shell.execute_reply.started":"2022-02-13T06:51:19.873616Z","shell.execute_reply":"2022-02-13T06:51:19.891796Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"mean length of question  8.787745136646452\n","output_type":"stream"}]},{"cell_type":"code","source":["plt.hist(lens_question)\n","plt.show()"],"metadata":{"id":"lvKNRNH_UN1h","executionInfo":{"status":"ok","timestamp":1641823686904,"user_tz":-180,"elapsed":323,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"fd80fdea-2d70-475a-95c1-6ddb0a61dd6f","execution":{"iopub.status.busy":"2022-02-13T06:51:19.894306Z","iopub.execute_input":"2022-02-13T06:51:19.896037Z","iopub.status.idle":"2022-02-13T06:51:20.161036Z","shell.execute_reply.started":"2022-02-13T06:51:19.895974Z","shell.execute_reply":"2022-02-13T06:51:20.160055Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["lens_passages= np.array([len(x.split()) for x in passages_corpus])\n","print('mean length of passage ', lens_passages.mean())"],"metadata":{"id":"8oAFFwBySIWx","executionInfo":{"status":"ok","timestamp":1641823226532,"user_tz":-180,"elapsed":274,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"a287c44b-d934-45d7-f027-de3c23b8b338","execution":{"iopub.status.busy":"2022-02-13T06:51:20.162972Z","iopub.execute_input":"2022-02-13T06:51:20.163396Z","iopub.status.idle":"2022-02-13T06:51:20.247460Z","shell.execute_reply.started":"2022-02-13T06:51:20.163355Z","shell.execute_reply":"2022-02-13T06:51:20.246165Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"mean length of passage  92.90989997637237\n","output_type":"stream"}]},{"cell_type":"code","source":["plt.hist(lens_passages)\n","plt.show()"],"metadata":{"id":"9aoLO_Ls4QDy","executionInfo":{"status":"ok","timestamp":1641823700388,"user_tz":-180,"elapsed":292,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"7e47aa80-2f4b-43ee-8685-8369d65528ba","execution":{"iopub.status.busy":"2022-02-13T06:51:20.252965Z","iopub.execute_input":"2022-02-13T06:51:20.253644Z","iopub.status.idle":"2022-02-13T06:51:20.488389Z","shell.execute_reply.started":"2022-02-13T06:51:20.253611Z","shell.execute_reply":"2022-02-13T06:51:20.487106Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Вопросы для датасета BoolQ собирались из массива анонимизированных аггрегированных запросов к поисковику Google. Отбирались вопросы достаточной длины, начинающиеся с эвристически отобранных слов (“did”, “do”, “does”, “is”, “are”, “was”, “were”, “have”, “has”, “can”, “could”, “will”, “would”) и содержащие в топ-5 поисковой выдачи ссылку на Википедию. В результате, как показано выше, сами вопросы имеют длину в среднем 9 слов."],"metadata":{"id":"mVK9B-88eNHD"}},{"cell_type":"code","source":["question_words = []\n","for question in questions_corpus:\n","  question_words.extend(question.split())\n","print('Total words in question corpus ', len(question_words))"],"metadata":{"id":"kJJ_lWmv2Y9R","executionInfo":{"status":"ok","timestamp":1641828339779,"user_tz":-180,"elapsed":264,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"26dec57f-2f8f-49ce-974f-8690d3f6289e","execution":{"iopub.status.busy":"2022-02-13T06:51:20.489795Z","iopub.execute_input":"2022-02-13T06:51:20.491332Z","iopub.status.idle":"2022-02-13T06:51:20.511966Z","shell.execute_reply.started":"2022-02-13T06:51:20.491281Z","shell.execute_reply":"2022-02-13T06:51:20.510689Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Total words in question corpus  111578\n","output_type":"stream"}]},{"cell_type":"code","source":["df_question = pd.DataFrame(question_words)\n","qvc = df_question.value_counts()"],"metadata":{"id":"dw_CRzSogX8c","execution":{"iopub.status.busy":"2022-02-13T06:51:20.513682Z","iopub.execute_input":"2022-02-13T06:51:20.514734Z","iopub.status.idle":"2022-02-13T06:51:20.574142Z","shell.execute_reply.started":"2022-02-13T06:51:20.514686Z","shell.execute_reply":"2022-02-13T06:51:20.573158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qvc[['no', 'not', 'don\\'t']]"],"metadata":{"id":"jU90dyTGkAgW","executionInfo":{"status":"ok","timestamp":1641827945993,"user_tz":-180,"elapsed":264,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"d1a2b12b-bb76-460b-f12f-504d6cc95397","execution":{"iopub.status.busy":"2022-02-13T06:51:20.575802Z","iopub.execute_input":"2022-02-13T06:51:20.576371Z","iopub.status.idle":"2022-02-13T06:51:20.594313Z","shell.execute_reply.started":"2022-02-13T06:51:20.576329Z","shell.execute_reply":"2022-02-13T06:51:20.593262Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"no       52\nnot      50\ndon't    13\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":["Видно, что отрицание редко встречается в вопросах. Авторы статьи отмечают, что в вопросах датасета редко встречаются вопросы о фактах, касающихся простых свойств сущностей. Например, можно показать, что конкретные численные значения редки."],"metadata":{"id":"ISiVzQ86kY_s"}},{"cell_type":"code","source":["regex_num = re.compile('\\d+')  \n","numbers = regex_num.findall(' '.join(question_words))\n","len(numbers) "],"metadata":{"id":"K4CrVK4ykv_W","executionInfo":{"status":"ok","timestamp":1641828164253,"user_tz":-180,"elapsed":3,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"067ebd95-4a40-4406-e400-f091b176ad14","execution":{"iopub.status.busy":"2022-02-13T06:51:20.595596Z","iopub.execute_input":"2022-02-13T06:51:20.598093Z","iopub.status.idle":"2022-02-13T06:51:20.630494Z","shell.execute_reply.started":"2022-02-13T06:51:20.598046Z","shell.execute_reply":"2022-02-13T06:51:20.629308Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"1842"},"metadata":{}}]},{"cell_type":"markdown","source":["## Часть 2. Baseline (1 балл)\n","1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n","2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n","\n","Почему fasttext плохо справляется с этой задачей?"],"metadata":{"id":"nRTuUReAqj4D"}},{"cell_type":"markdown","source":["**Константное решение**"],"metadata":{"id":"2RKfZxF5mMjM"}},{"cell_type":"code","source":["const_pred = np.bincount(answers_train).argmax()\n","const_pred = np.ones_like(answers_dev) * const_pred\n","print('Accuracy score for constant model: ', accuracy_score(answers_dev, const_pred))"],"metadata":{"id":"zD2DVzB8mXVE","executionInfo":{"status":"ok","timestamp":1641828957086,"user_tz":-180,"elapsed":5,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"d87878d4-9731-433c-a102-18ffac307c43","execution":{"iopub.status.busy":"2022-02-13T06:51:20.632216Z","iopub.execute_input":"2022-02-13T06:51:20.632907Z","iopub.status.idle":"2022-02-13T06:51:20.644578Z","shell.execute_reply.started":"2022-02-13T06:51:20.632847Z","shell.execute_reply":"2022-02-13T06:51:20.643375Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Accuracy score for constant model:  0.6217125382262997\n","output_type":"stream"}]},{"cell_type":"markdown","source":["**Fasttext**"],"metadata":{"id":"6x_W4xl2odnP"}},{"cell_type":"code","source":["!pip3 install fasttext"],"metadata":{"id":"mmgm7nJh9hb8","executionInfo":{"status":"ok","timestamp":1641910326553,"user_tz":-180,"elapsed":49761,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"e07188b9-8e90-4b2c-bb77-ef221226e773","execution":{"iopub.status.busy":"2022-02-13T06:51:20.646554Z","iopub.execute_input":"2022-02-13T06:51:20.646957Z","iopub.status.idle":"2022-02-13T06:51:29.878106Z","shell.execute_reply.started":"2022-02-13T06:51:20.646913Z","shell.execute_reply":"2022-02-13T06:51:29.876986Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fasttext in /opt/conda/lib/python3.7/site-packages (0.9.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext) (1.20.3)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext) (2.9.1)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext) (59.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":["import fasttext\n","import nltk\n","from string import punctuation\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize\n","from nltk import WordPunctTokenizer\n","import re\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","en_stop = set(nltk.corpus.stopwords.words('english'))\n","\n","%matplotlib inline"],"metadata":{"id":"QdKYj2on9Qhv","executionInfo":{"status":"ok","timestamp":1641910331263,"user_tz":-180,"elapsed":4714,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"3ea727cb-f704-44d8-e3ad-17203e6fb5db","execution":{"iopub.status.busy":"2022-02-13T06:51:29.881810Z","iopub.execute_input":"2022-02-13T06:51:29.882062Z","iopub.status.idle":"2022-02-13T06:51:30.167315Z","shell.execute_reply.started":"2022-02-13T06:51:29.882032Z","shell.execute_reply":"2022-02-13T06:51:30.166396Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":["stemmer = WordNetLemmatizer()\n","\n","def preprocess_text(document):\n","        # Уберем пунктуацию и спецсимволы\n","        document = re.sub(r'\\W', ' ', str(document))\n","        # Заменим множественные пробелы на один\n","        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","        # Преобразуем в нижнгий регистр\n","        document = document.lower()\n","        # Уберем стоп-слова, короткие слова и лемматизируем\n","        tokens = document.split()\n","        tokens = [stemmer.lemmatize(word) for word in tokens]\n","        tokens = [word for word in tokens if word not in en_stop]\n","        tokens = [word for word in tokens if len(word) > 2]\n","\n","        preprocessed_text = ' '.join(tokens)\n","\n","        return preprocessed_text"],"metadata":{"id":"1gcneuirdlpB","execution":{"iopub.status.busy":"2022-02-13T06:51:30.170150Z","iopub.execute_input":"2022-02-13T06:51:30.171072Z","iopub.status.idle":"2022-02-13T06:51:30.178352Z","shell.execute_reply.started":"2022-02-13T06:51:30.171024Z","shell.execute_reply":"2022-02-13T06:51:30.177224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Подготовим данные\n","passages_val, passages_test, questions_val, questions_test, answers_val, answers_test = \\\n","  train_test_split(passages_dev, questions_dev, answers_dev, test_size=0.5)"],"metadata":{"id":"6i16vhbooiE2","execution":{"iopub.status.busy":"2022-02-13T06:51:30.180204Z","iopub.execute_input":"2022-02-13T06:51:30.180977Z","iopub.status.idle":"2022-02-13T06:51:30.201271Z","shell.execute_reply.started":"2022-02-13T06:51:30.180917Z","shell.execute_reply":"2022-02-13T06:51:30.200215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_prepare (passages, questions, answers, mode = 'train'):\n","  snts = []\n","  trgs = []\n","  for i in range(len(passages)):\n","    snt = preprocess_text(' '.join([questions[i], passages[i]]))\n","    trg = '__label__' + str(answers[i])\n","    snts.append(snt)\n","    trgs.append(trg)\n","  df = pd.DataFrame({'txt': snts, 'target': trgs})\n","  df[['target', 'txt']].to_csv(mode + '_data.txt', header=False, index=False, sep=\"\\t\")"],"metadata":{"id":"9ift5fIAermD","execution":{"iopub.status.busy":"2022-02-13T06:51:30.202835Z","iopub.execute_input":"2022-02-13T06:51:30.203444Z","iopub.status.idle":"2022-02-13T06:51:30.215125Z","shell.execute_reply.started":"2022-02-13T06:51:30.203398Z","shell.execute_reply":"2022-02-13T06:51:30.213994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_prepare (passages_train, questions_train, answers_train, mode = 'train')\n","data_prepare (passages_val, questions_val, answers_val, mode = 'val')"],"metadata":{"id":"AxMDGZY2gfFx","execution":{"iopub.status.busy":"2022-02-13T06:51:30.216585Z","iopub.execute_input":"2022-02-13T06:51:30.219015Z","iopub.status.idle":"2022-02-13T06:51:40.147024Z","shell.execute_reply.started":"2022-02-13T06:51:30.218969Z","shell.execute_reply":"2022-02-13T06:51:40.145856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = fasttext.train_supervised('train_data.txt', autotuneValidationFile='val_data.txt', autotuneMetric=\"f1:__label__1\")"],"metadata":{"id":"Af8Zdsp5hUe7","outputId":"cff36248-e326-42b3-d274-b80d9c826df8","execution":{"iopub.status.busy":"2022-02-13T06:51:40.148855Z","iopub.execute_input":"2022-02-13T06:51:40.149146Z","iopub.status.idle":"2022-02-13T06:56:42.414137Z","shell.execute_reply.started":"2022-02-13T06:51:40.149103Z","shell.execute_reply":"2022-02-13T06:56:42.413129Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Progress: 100.0% Trials:   23 Best score:  0.777778 ETA:   0h 0m 0s\nTraining again with best arguments\nRead 0M words\nNumber of words:  37305\nNumber of labels: 2\nProgress: 100.0% words/sec/thread: 1261874 lr:  0.000000 avg.loss:  0.625655 ETA:   0h 0m 0s\n","output_type":"stream"}]},{"cell_type":"code","source":["ft_pred = []\n","for i in range(len(passages_test)):\n","  snt = preprocess_text(' '.join([questions_test[i], passages_test[i]]))\n","  p = model.predict(snt, k=2)\n","  label = 0\n","  ft_pred.append(model.predict(snt, k=2)[1][0] > 0.5)"],"metadata":{"id":"02gBNcb4h6HS","execution":{"iopub.status.busy":"2022-02-13T06:56:42.420987Z","iopub.execute_input":"2022-02-13T06:56:42.424831Z","iopub.status.idle":"2022-02-13T06:56:44.137988Z","shell.execute_reply.started":"2022-02-13T06:56:42.424788Z","shell.execute_reply":"2022-02-13T06:56:44.136993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Accuracy score for FastTEXT model: ', accuracy_score(answers_test, ft_pred))"],"metadata":{"id":"uRt_tbrM_aBF","executionInfo":{"status":"ok","timestamp":1641912435670,"user_tz":-180,"elapsed":419,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"874aaabf-4806-4499-bb87-ec7b68d7738a","execution":{"iopub.status.busy":"2022-02-13T06:56:44.139541Z","iopub.execute_input":"2022-02-13T06:56:44.139811Z","iopub.status.idle":"2022-02-13T06:56:44.147671Z","shell.execute_reply.started":"2022-02-13T06:56:44.139772Z","shell.execute_reply":"2022-02-13T06:56:44.146453Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Accuracy score for FastTEXT model:  0.6281345565749236\n","output_type":"stream"}]},{"cell_type":"markdown","source":["FastTEXT плохо справляется с задачей, поскольку не учитывает позиции токенов. Эта модель больше подходит для тематической классификации текстов."],"metadata":{"id":"hF4rdAB6oCI2"}},{"cell_type":"markdown","source":["## Часть 3. Используем эмбеддинги предложений (2 балла)\n","1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n","\n","[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n","\n","[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. "],"metadata":{"id":"Fg84O0bfqj4G"}},{"cell_type":"markdown","source":["Загрузим предобученный БЕРТ с токенайзером."],"metadata":{"id":"K-Pj8o0kzM3_"}},{"cell_type":"code","source":["model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)"],"metadata":{"id":"wOEpVVoCsU-3","executionInfo":{"status":"ok","timestamp":1641998851735,"user_tz":-180,"elapsed":17557,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"da7815bd-6893-47d9-8c4c-b3db6aa235a5","execution":{"iopub.status.busy":"2022-02-13T06:56:44.149133Z","iopub.execute_input":"2022-02-13T06:56:44.150140Z","iopub.status.idle":"2022-02-13T06:57:09.733203Z","shell.execute_reply.started":"2022-02-13T06:56:44.150092Z","shell.execute_reply":"2022-02-13T06:57:09.732074Z"},"trusted":true,"colab":{"referenced_widgets":["82b155e3eb544783bf1ce8b7c38e030c","5515d362935649f6947f929de0c8d135","d3bac08e36dd4319a85c562c1a4f1488","86a24a7bb712454c9cd950dd0a9ec8e4","dd67b0cd6e3a437b8ded156eb726acb2"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b155e3eb544783bf1ce8b7c38e030c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5515d362935649f6947f929de0c8d135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bac08e36dd4319a85c562c1a4f1488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a24a7bb712454c9cd950dd0a9ec8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd67b0cd6e3a437b8ded156eb726acb2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":["model = model.to(device)\n","model.eval()"],"metadata":{"id":"eh-_uRpAsjbw","executionInfo":{"status":"ok","timestamp":1641998860469,"user_tz":-180,"elapsed":8744,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"c5ac65fc-af02-4f12-936a-f1201829d540","execution":{"iopub.status.busy":"2022-02-13T06:57:09.735596Z","iopub.execute_input":"2022-02-13T06:57:09.736246Z","iopub.status.idle":"2022-02-13T06:57:13.123467Z","shell.execute_reply.started":"2022-02-13T06:57:09.736200Z","shell.execute_reply":"2022-02-13T06:57:13.122423Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":["Подготовим класс для работы с данными."],"metadata":{"id":"HNqhF4hxzM3_"}},{"cell_type":"code","source":["class QADataset(Dataset):\n","    def __init__(self, passages, questions, answers, tokenizer):\n","        self.labels = answers\n","        # tokenization\n","        self.passages = passages.apply((lambda x: x if (len(x) <= 512) else x[:512]))\n","        self.passages = self.passages.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n","        self.questions = questions.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n","        \n","    def __getitem__(self, idx):\n","        return {\"passages\": self.passages[idx], \"questions\": self.questions[idx], \"labels\": self.labels[idx]}\n","\n","    def __len__(self):\n","        return len(self.labels)\n"],"metadata":{"id":"tAGcnjGXsjeH","execution":{"iopub.status.busy":"2022-02-13T06:57:13.125250Z","iopub.execute_input":"2022-02-13T06:57:13.125736Z","iopub.status.idle":"2022-02-13T06:57:13.136275Z","shell.execute_reply.started":"2022-02-13T06:57:13.125676Z","shell.execute_reply":"2022-02-13T06:57:13.134868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Загрузим данные в датасеты, разобъем dev на валидационную и тестовую части."],"metadata":{"id":"aVJuQ594zM4A"}},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","print(f\"Number of training examples: {len(train_data)}\")\n","print(f\"Number of validation examples: {len(valid_data)}\")\n","print(f\"Number of testing examples: {len(test_data)}\")"],"metadata":{"id":"WocmsPb_hAk9","outputId":"c2fff78b-7078-402d-84d8-fa3143851a52","execution":{"iopub.status.busy":"2022-02-13T06:57:13.137639Z","iopub.execute_input":"2022-02-13T06:57:13.138600Z","iopub.status.idle":"2022-02-13T06:57:57.673215Z","shell.execute_reply.started":"2022-02-13T06:57:13.138553Z","shell.execute_reply":"2022-02-13T06:57:57.672060Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of training examples: 9427\nNumber of validation examples: 1635\nNumber of testing examples: 1635\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Подготовим итераторы."],"metadata":{"id":"HwYgEYT8zM4A"}},{"cell_type":"code","source":["class QASampler(Sampler):\n","    def __init__(self, subset, batch_size=32):\n","        self.batch_size = batch_size\n","        self.subset = subset\n","        \n","        self.indices = subset.indices\n","\n","        self.passages = np.array(subset.dataset.passages)[self.indices]\n","\n","    def __iter__(self):\n","\n","        batch_idx = []\n","        # index in sorted data\n","        for index in np.argsort(list(map(len, self.passages))):\n","            batch_idx.append(index)\n","            if len(batch_idx) == self.batch_size:\n","                yield batch_idx\n","                batch_idx = []\n","\n","        if len(batch_idx) > 0:\n","            yield batch_idx\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"metadata":{"id":"-l9nS2aCsjgg","execution":{"iopub.status.busy":"2022-02-13T06:57:57.675113Z","iopub.execute_input":"2022-02-13T06:57:57.675800Z","iopub.status.idle":"2022-02-13T06:57:57.685290Z","shell.execute_reply.started":"2022-02-13T06:57:57.675749Z","shell.execute_reply":"2022-02-13T06:57:57.684190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_padded(values):\n","    max_len = 0\n","    for value in values:\n","        if len(value) > max_len:\n","            max_len = len(value)\n","\n","    padded = np.array([value + [0]*(max_len-len(value)) for value in values])\n","\n","    return padded\n","\n","def collate_fn(batch):\n","\n","    passages = []\n","    questions = []\n","    labels = []\n","    for elem in batch:\n","        passages.append(elem['passages'])\n","        questions.append(elem['questions'])\n","        labels.append(elem['labels'])\n","\n","    questions = get_padded(questions)\n","    attention_mask_q = np.where(questions != 0, 1, 0)\n","\n","    passages = get_padded(passages)\n","    attention_mask_p = np.where(passages != 0, 1, 0)\n","\n","    return {'passages': torch.tensor(passages), 'questions': torch.tensor(questions), \n","            'labels': torch.FloatTensor(labels), \n","            'attention_mask_p' : torch.tensor(attention_mask_p), 'attention_mask_q' : torch.tensor(attention_mask_q)}"],"metadata":{"id":"y0PDATGFymOI","execution":{"iopub.status.busy":"2022-02-13T06:57:57.687258Z","iopub.execute_input":"2022-02-13T06:57:57.687653Z","iopub.status.idle":"2022-02-13T06:57:57.704416Z","shell.execute_reply.started":"2022-02-13T06:57:57.687610Z","shell.execute_reply":"2022-02-13T06:57:57.703238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)"],"metadata":{"id":"LTHFhqvOhiln","execution":{"iopub.status.busy":"2022-02-13T06:57:57.706345Z","iopub.execute_input":"2022-02-13T06:57:57.707234Z","iopub.status.idle":"2022-02-13T06:57:57.718888Z","shell.execute_reply.started":"2022-02-13T06:57:57.707117Z","shell.execute_reply":"2022-02-13T06:57:57.717944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Подготовим функцию, выдающую на основе итератора признаки (эмбеддинги предобученных трансформеров) и метки для последующей передачи в логистическую регрессию."],"metadata":{"id":"dqiPhPtIzM4B"}},{"cell_type":"code","source":["def get_xy(loader):\n","    features = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            \n","            questions = batch['questions'].to(device)\n","            passages = batch['passages'].to(device)\n","            attention_mask_p = batch['attention_mask_p'].to(device)\n","            attention_mask_q = batch['attention_mask_q'].to(device)\n","            label = batch['labels']\n","\n","            last_hidden_states_p = model(passages, attention_mask=attention_mask_p)\n","            last_hidden_states_q = model(questions, attention_mask=attention_mask_q)\n","            \n","            features.append(torch.cat([last_hidden_states_q[0][:, 0, :], last_hidden_states_p[0][:, 0, :]], dim=1).cpu())\n","            labels.append(label.cpu())\n","\n","    features = torch.cat(features, dim=0).numpy()\n","    labels = torch.cat(labels, dim=0).numpy()\n","    \n","    return features, labels"],"metadata":{"id":"2v4vciHnymWL","execution":{"iopub.status.busy":"2022-02-13T06:57:57.721217Z","iopub.execute_input":"2022-02-13T06:57:57.721916Z","iopub.status.idle":"2022-02-13T06:57:57.733011Z","shell.execute_reply.started":"2022-02-13T06:57:57.721871Z","shell.execute_reply":"2022-02-13T06:57:57.731952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"FkrCMrSRymfG","executionInfo":{"status":"ok","timestamp":1642000125015,"user_tz":-180,"elapsed":169128,"user":{"displayName":"Victor Shlyakhin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10685837151006696829"}},"outputId":"f819a562-b367-4f34-82cf-a80f547019d5","execution":{"iopub.status.busy":"2022-02-13T06:57:57.734938Z","iopub.execute_input":"2022-02-13T06:57:57.735694Z","iopub.status.idle":"2022-02-13T06:58:44.812391Z","shell.execute_reply.started":"2022-02-13T06:57:57.735648Z","shell.execute_reply":"2022-02-13T06:58:44.811473Z"},"trusted":true,"colab":{"referenced_widgets":["88fb2ca5cf06414982271d12f26612bf","f6d185ede2824cbcad44ca59c1443c83","a3581cb3e89144fbb9ea616a2b8ecb50"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88fb2ca5cf06414982271d12f26612bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6d185ede2824cbcad44ca59c1443c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3581cb3e89144fbb9ea616a2b8ecb50"}},"metadata":{}}]},{"cell_type":"markdown","source":["Обучим и проверим логистическую регрессию. Гиперпараметры логрега подбирались на этапе экспериментов с использованием GridSearchCV."],"metadata":{"id":"g-unO2enzM4G"}},{"cell_type":"code","source":["clf = LogisticRegression(C=0.005, penalty='l2', solver='saga')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"1ZazXIBxzM4G","outputId":"30743b0e-86b0-40c6-a856-c098c358ab73","execution":{"iopub.status.busy":"2022-02-13T06:58:44.814370Z","iopub.execute_input":"2022-02-13T06:58:44.814810Z","iopub.status.idle":"2022-02-13T06:59:12.519540Z","shell.execute_reply.started":"2022-02-13T06:58:44.814765Z","shell.execute_reply":"2022-02-13T06:59:12.518448Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0.655045871559633"},"metadata":{}}]},{"cell_type":"markdown","source":["Результат - 0.6550."],"metadata":{"id":"h_nqOF5kzM4G"}},{"cell_type":"markdown","source":["Попробуем большую модель Берт"],"metadata":{"id":"as9Q1I4uzM4G"}},{"cell_type":"code","source":["model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-large-uncased-whole-word-masking-finetuned-squad')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"P16Ps1bWhrxO","outputId":"9d3558fc-e06e-44ed-f10d-c3c247126c7f","execution":{"iopub.status.busy":"2022-02-13T06:59:12.525280Z","iopub.execute_input":"2022-02-13T06:59:12.528037Z","iopub.status.idle":"2022-02-13T07:00:12.347184Z","shell.execute_reply.started":"2022-02-13T06:59:12.527986Z","shell.execute_reply":"2022-02-13T07:00:12.346100Z"},"trusted":true,"colab":{"referenced_widgets":["006fb3efbd244504aa750174b7ef1f66","da3892ef6780404cb2fba238b59779b5","88d900c9d70941cb984c083af61391b2","cee3cce6b096428e8b37daa5f7df5ae5","87c01449361f43e6b992f526246be088"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006fb3efbd244504aa750174b7ef1f66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3892ef6780404cb2fba238b59779b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d900c9d70941cb984c083af61391b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee3cce6b096428e8b37daa5f7df5ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c01449361f43e6b992f526246be088"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n    (position_embeddings): Embedding(512, 1024)\n    (token_type_embeddings): Embedding(2, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":["Заново подготовим данные (с учетом возможного отличия токенайзера)."],"metadata":{"id":"V4qZCMX1zM4H"}},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)\n","\n","train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"Flevq45XzM4H","outputId":"04816ba8-d3e9-485b-eb89-bd9735f29ef1","execution":{"iopub.status.busy":"2022-02-13T07:00:12.349204Z","iopub.execute_input":"2022-02-13T07:00:12.349538Z","iopub.status.idle":"2022-02-13T07:03:18.598746Z","shell.execute_reply.started":"2022-02-13T07:00:12.349492Z","shell.execute_reply":"2022-02-13T07:03:18.597445Z"},"trusted":true,"colab":{"referenced_widgets":["b3d1b05deec74550bd67bbf8af1bc58b","668c509eee4949feb435b37aa48ce283","4ae7d02a20a54b8d8bdf712f02902f02"]}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d1b05deec74550bd67bbf8af1bc58b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668c509eee4949feb435b37aa48ce283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae7d02a20a54b8d8bdf712f02902f02"}},"metadata":{}}]},{"cell_type":"code","source":["clf = LogisticRegression(C=0.05, penalty='l2', solver='liblinear')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"8dvhBMvfzM4H","execution":{"iopub.status.busy":"2022-02-13T07:03:18.605973Z","iopub.execute_input":"2022-02-13T07:03:18.606337Z","iopub.status.idle":"2022-02-13T07:03:26.465772Z","shell.execute_reply.started":"2022-02-13T07:03:18.606304Z","shell.execute_reply":"2022-02-13T07:03:26.464636Z"},"trusted":true,"outputId":"2f0dca0c-789f-48a1-e0be-d7430b8e1bba"},"execution_count":null,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"0.6770642201834862"},"metadata":{}}]},{"cell_type":"markdown","source":["Результат - 0.6771"],"metadata":{"id":"D4XBorNnzM4H"}},{"cell_type":"markdown","source":["Возьмём roBRETa"],"metadata":{"id":"2Z2IE1pTzM4I"}},{"cell_type":"code","source":["model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel,ppb.RobertaTokenizer, 'roberta-large')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"E6_MNMv0zM4I","execution":{"iopub.status.busy":"2022-02-13T07:03:26.467326Z","iopub.execute_input":"2022-02-13T07:03:26.471869Z","iopub.status.idle":"2022-02-13T07:04:31.501286Z","shell.execute_reply.started":"2022-02-13T07:03:26.471800Z","shell.execute_reply":"2022-02-13T07:04:31.500241Z"},"trusted":true,"colab":{"referenced_widgets":["e1ef242eb9034e6b90c81780d7522dbb","eafde6f845334958a06e0cc1c7bedef9","a7e2d329c86f40ba885b2ea212f63932","959638a9cd824b4cb00baa5d487329d6","bb2ee0306b024829a0b3c0fe5c9f2c53"]},"outputId":"eb06ee17-cc46-4541-be0a-a51fd7a8f660"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ef242eb9034e6b90c81780d7522dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eafde6f845334958a06e0cc1c7bedef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e2d329c86f40ba885b2ea212f63932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959638a9cd824b4cb00baa5d487329d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2ee0306b024829a0b3c0fe5c9f2c53"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)\n","\n","train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"ZI6fSrKJzM4I","execution":{"iopub.status.busy":"2022-02-13T07:04:31.504389Z","iopub.execute_input":"2022-02-13T07:04:31.505017Z","iopub.status.idle":"2022-02-13T07:07:15.048831Z","shell.execute_reply.started":"2022-02-13T07:04:31.504969Z","shell.execute_reply":"2022-02-13T07:07:15.047843Z"},"trusted":true,"colab":{"referenced_widgets":["47eba4972d4c49009e64caf483a46c5b","9d15dde9bf164e56b892b6488216e85b","43c06d55b6ac4d0b9e773771fa13f690"]},"outputId":"769f9f02-a5fb-4ed1-b102-3e151a40c2a5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47eba4972d4c49009e64caf483a46c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d15dde9bf164e56b892b6488216e85b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c06d55b6ac4d0b9e773771fa13f690"}},"metadata":{}}]},{"cell_type":"code","source":["clf = LogisticRegression(C=0.1, penalty='l2', solver='liblinear')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"lmPiDSq4zM4I","execution":{"iopub.status.busy":"2022-02-13T07:07:15.050705Z","iopub.execute_input":"2022-02-13T07:07:15.051030Z","iopub.status.idle":"2022-02-13T07:07:23.374939Z","shell.execute_reply.started":"2022-02-13T07:07:15.050985Z","shell.execute_reply":"2022-02-13T07:07:23.373703Z"},"trusted":true,"outputId":"513a5f17-46a1-427b-d238-04b6354251fe"},"execution_count":null,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"0.6568807339449542"},"metadata":{}}]},{"cell_type":"markdown","source":["Результат - 0.6569"],"metadata":{"id":"eNmcH5pvzM4I"}},{"cell_type":"markdown","source":["Пробуем также ELECTRA."],"metadata":{"id":"ltmuACuyzM4J"}},{"cell_type":"code","source":["# 'google/electra-small-discriminator'\n","model_class, tokenizer_class, pretrained_weights = (ppb.ElectraModel,ppb.ElectraTokenizer, 'google/electra-large-generator')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"TPUhwpiQzM4J","execution":{"iopub.status.busy":"2022-02-13T07:07:23.376771Z","iopub.execute_input":"2022-02-13T07:07:23.377078Z","iopub.status.idle":"2022-02-13T07:07:39.949678Z","shell.execute_reply.started":"2022-02-13T07:07:23.377034Z","shell.execute_reply":"2022-02-13T07:07:39.948649Z"},"trusted":true,"colab":{"referenced_widgets":["0ed1c54aef844d418797a311ac8a0987","7d54cbf3798244479e4264425c7d0255","7b9e21bb43bf4935a45eb27758c63181","449fa714f064437fa86e4d68cb40743b","813bdee429964bb09f4499f65edff7b4"]},"outputId":"462b85a5-ce35-49b5-d951-1e415ac558a0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed1c54aef844d418797a311ac8a0987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d54cbf3798244479e4264425c7d0255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b9e21bb43bf4935a45eb27758c63181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449fa714f064437fa86e4d68cb40743b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/196M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"813bdee429964bb09f4499f65edff7b4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/electra-large-generator were not used when initializing ElectraModel: ['generator_lm_head.weight', 'generator_lm_head.bias', 'generator_predictions.dense.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.bias']\n- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n    (position_embeddings): Embedding(512, 1024)\n    (token_type_embeddings): Embedding(2, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=1024, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)\n","\n","train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"TvVLGdhlzM4J","execution":{"iopub.status.busy":"2022-02-13T07:07:39.954675Z","iopub.execute_input":"2022-02-13T07:07:39.962306Z","iopub.status.idle":"2022-02-13T07:08:47.738919Z","shell.execute_reply.started":"2022-02-13T07:07:39.962259Z","shell.execute_reply":"2022-02-13T07:08:47.737559Z"},"trusted":true,"colab":{"referenced_widgets":["e3581d0055d94d379fbded1fe084644d","6614b0a465164fc889e434fe4a9add02","0928109bcde54fbeac7fece1133e485d"]},"outputId":"a1153418-9437-45ad-a4fc-e61892ab07c9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3581d0055d94d379fbded1fe084644d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6614b0a465164fc889e434fe4a9add02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0928109bcde54fbeac7fece1133e485d"}},"metadata":{}}]},{"cell_type":"code","source":["clf = LogisticRegression(C=15, penalty='l2', solver='liblinear')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"di_wqoc4zM4J","execution":{"iopub.status.busy":"2022-02-13T07:08:47.740560Z","iopub.execute_input":"2022-02-13T07:08:47.740969Z","iopub.status.idle":"2022-02-13T07:08:51.768700Z","shell.execute_reply.started":"2022-02-13T07:08:47.740926Z","shell.execute_reply":"2022-02-13T07:08:51.767697Z"},"trusted":true,"outputId":"1e617af5-d2a4-4523-fae2-ce353ef44805"},"execution_count":null,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"0.6495412844036698"},"metadata":{}}]},{"cell_type":"markdown","source":["Результат - 0.6495"],"metadata":{"id":"5pZ7JaL0zM4J"}},{"cell_type":"markdown","source":["И ещё ALBERT."],"metadata":{"id":"UslY-VivzM4J"}},{"cell_type":"code","source":["# 'albert-base-v2'\n","model_class, tokenizer_class, pretrained_weights = (ppb.AlbertModel,ppb.AlbertTokenizer, 'albert-xxlarge-v2')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"FNsEWizAzM4J","execution":{"iopub.status.busy":"2022-02-13T07:08:51.774438Z","iopub.execute_input":"2022-02-13T07:08:51.777370Z","iopub.status.idle":"2022-02-13T07:09:34.171082Z","shell.execute_reply.started":"2022-02-13T07:08:51.777319Z","shell.execute_reply":"2022-02-13T07:09:34.170073Z"},"trusted":true,"colab":{"referenced_widgets":["972929dee5c644a7873393d89451e7d3","10f4a279d5f1443db6f32c87ce333a5d","87380817698944adbbe53d87f03ce398","1b361ec40cc44c009047639cb3702644"]},"outputId":"26a63948-f8c0-49c7-fc09-6a4f82db2945"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/742k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972929dee5c644a7873393d89451e7d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f4a279d5f1443db6f32c87ce333a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/710 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87380817698944adbbe53d87f03ce398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/851M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b361ec40cc44c009047639cb3702644"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"AlbertModel(\n  (embeddings): AlbertEmbeddings(\n    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (encoder): AlbertTransformer(\n    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n    (albert_layer_groups): ModuleList(\n      (0): AlbertLayerGroup(\n        (albert_layers): ModuleList(\n          (0): AlbertLayer(\n            (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n            (attention): AlbertAttention(\n              (query): Linear(in_features=4096, out_features=4096, bias=True)\n              (key): Linear(in_features=4096, out_features=4096, bias=True)\n              (value): Linear(in_features=4096, out_features=4096, bias=True)\n              (attention_dropout): Dropout(p=0, inplace=False)\n              (output_dropout): Dropout(p=0, inplace=False)\n              (dense): Linear(in_features=4096, out_features=4096, bias=True)\n              (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n            )\n            (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n            (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (pooler): Linear(in_features=4096, out_features=4096, bias=True)\n  (pooler_activation): Tanh()\n)"},"metadata":{}}]},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)\n","\n","train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"o4M_t0OJzM4K","execution":{"iopub.status.busy":"2022-02-13T07:09:34.172721Z","iopub.execute_input":"2022-02-13T07:09:34.173280Z","iopub.status.idle":"2022-02-13T07:25:37.971960Z","shell.execute_reply.started":"2022-02-13T07:09:34.173235Z","shell.execute_reply":"2022-02-13T07:25:37.970983Z"},"trusted":true,"colab":{"referenced_widgets":["9d6d5aa1c28741a991f05504b23a74e1","e55577c45bf74b93b9188c5e152507e9","a67e986a987843caa3eb606c0b9fad85"]},"outputId":"1c948c80-d71e-4457-d647-3e56c942310c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6d5aa1c28741a991f05504b23a74e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e55577c45bf74b93b9188c5e152507e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67e986a987843caa3eb606c0b9fad85"}},"metadata":{}}]},{"cell_type":"code","source":["clf = LogisticRegression(C=0.07, penalty='l2', solver='liblinear')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"G3Woyx-UzM4K","execution":{"iopub.status.busy":"2022-02-13T07:25:37.973417Z","iopub.execute_input":"2022-02-13T07:25:37.974396Z","iopub.status.idle":"2022-02-13T07:26:27.180642Z","shell.execute_reply.started":"2022-02-13T07:25:37.974347Z","shell.execute_reply":"2022-02-13T07:26:27.179502Z"},"trusted":true,"outputId":"53ea29a9-8058-4645-cccf-e772bd090ffe"},"execution_count":null,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"0.6629969418960244"},"metadata":{}}]},{"cell_type":"markdown","source":["Результат - 0.6630"],"metadata":{"id":"uMDOTrDrzM4K"}},{"cell_type":"markdown","source":["Лучший результат (accuracy=0.6771) показало использование эмбеддингов BERT_Large. Он выше бейзлайна, но не сильно. Проверим модель на подготовленном примере."],"metadata":{"id":"dLc7sl91zM4K"}},{"cell_type":"code","source":["model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-large-uncased-whole-word-masking-finetuned-squad')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"FuR07gNtf7Y4","execution":{"iopub.status.busy":"2022-02-13T07:26:27.182539Z","iopub.execute_input":"2022-02-13T07:26:27.182880Z","iopub.status.idle":"2022-02-13T07:26:37.945094Z","shell.execute_reply.started":"2022-02-13T07:26:27.182838Z","shell.execute_reply":"2022-02-13T07:26:37.944208Z"},"trusted":true,"outputId":"de9b9a1b-8630-48b5-e6c0-647b7c245712"},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n    (position_embeddings): Embedding(512, 1024)\n    (token_type_embeddings): Embedding(2, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":["train_data = QADataset(train_data_df['passage'], train_data_df['question'], train_data_df['answer'],  tokenizer)\n","dev_data = QADataset(dev_data_df['passage'], dev_data_df['question'], dev_data_df['answer'],  tokenizer)\n","\n","train_data = Subset(train_data, range(len(train_data)))\n","val_size = int(.5 * len(dev_data)) \n","valid_data, test_data = random_split(dev_data, [val_size, len(dev_data) - val_size])\n","\n","train_loader = DataLoader(train_data, batch_sampler=QASampler(train_data), collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_sampler=QASampler(valid_data), collate_fn=collate_fn)\n","test_loader = DataLoader(test_data, batch_sampler=QASampler(test_data), collate_fn=collate_fn)\n","\n","train_features, train_labels = get_xy(train_loader)\n","valid_features, valid_labels = get_xy(valid_loader)\n","test_features, test_labels = get_xy(test_loader)"],"metadata":{"id":"JkVEGo1czM4L","execution":{"iopub.status.busy":"2022-02-13T07:26:37.946559Z","iopub.execute_input":"2022-02-13T07:26:37.947045Z","iopub.status.idle":"2022-02-13T07:29:42.575773Z","shell.execute_reply.started":"2022-02-13T07:26:37.947004Z","shell.execute_reply":"2022-02-13T07:29:42.574582Z"},"trusted":true,"colab":{"referenced_widgets":["ca4ef506ff1047b38503e067f502f4e1","603c27da553f43eab2110a8ccd1ea436","923bd1b9c4a3481990f5ca9aade6d3e5"]},"outputId":"b42a6d07-a686-4770-8071-048412d27795"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4ef506ff1047b38503e067f502f4e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603c27da553f43eab2110a8ccd1ea436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923bd1b9c4a3481990f5ca9aade6d3e5"}},"metadata":{}}]},{"cell_type":"code","source":["clf = LogisticRegression(C=0.05, penalty='l2', solver='liblinear')\n","clf.fit(train_features, train_labels)\n","clf.score(test_features, test_labels)"],"metadata":{"id":"W0MBf4ijzM4L","execution":{"iopub.status.busy":"2022-02-13T07:29:42.577663Z","iopub.execute_input":"2022-02-13T07:29:42.578001Z","iopub.status.idle":"2022-02-13T07:29:50.143197Z","shell.execute_reply.started":"2022-02-13T07:29:42.577950Z","shell.execute_reply":"2022-02-13T07:29:50.142162Z"},"trusted":true,"outputId":"e37c04a5-8d49-43cb-b53e-39edee7ab39f"},"execution_count":null,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"0.6678899082568808"},"metadata":{}}]},{"cell_type":"markdown","source":["Посмотрим ещё f1-метрику."],"metadata":{"id":"LZZpEP7CzM4L"}},{"cell_type":"code","source":["preds = clf.predict(test_features)\n","print('F1-score: ', f1_score(test_labels, preds))"],"metadata":{"id":"UuMqoKAgzM4L","execution":{"iopub.status.busy":"2022-02-13T07:29:50.144869Z","iopub.execute_input":"2022-02-13T07:29:50.145799Z","iopub.status.idle":"2022-02-13T07:29:50.171134Z","shell.execute_reply.started":"2022-02-13T07:29:50.145743Z","shell.execute_reply":"2022-02-13T07:29:50.170084Z"},"trusted":true,"outputId":"9cc44bc0-0ed4-4427-a8e9-22f43aef0bff"},"execution_count":null,"outputs":[{"name":"stdout","text":"F1-score:  0.7574810183117464\n","output_type":"stream"}]},{"cell_type":"code","source":["def predict (model, tokenizer, clf, questions, passages):\n","  \n","    for i in range(len(questions)):\n","        question = tokenizer.encode(questions[i], add_special_tokens=True)\n","        question = torch.LongTensor(question).unsqueeze(0).to(device)\n","        attention_mask_q = torch.ones_like(question).to(device)\n","\n","        passage = tokenizer.encode(passages[i], add_special_tokens=True)\n","        passage = torch.LongTensor(passage).unsqueeze(0).to(device)\n","        attention_mask_p = torch.ones_like(passage).to(device)\n","\n","        last_hidden_states_p = model(passage, attention_mask=attention_mask_p)\n","        last_hidden_states_q = model(question, attention_mask=attention_mask_q)\n","\n","        features = torch.cat([last_hidden_states_q[0][:, 0, :], last_hidden_states_p[0][:, 0, :]], dim=1).cpu().detach().numpy()\n","\n","        pred = clf.predict(features)\n","\n","\n","\n","        print(\"Q.: \", questions[i])\n","        # print(\"P.: \", passages[i])\n","        if pred:\n","            print ('YES')\n","        else:\n","            print('NO')"],"metadata":{"id":"0HwjoB1azM4L","execution":{"iopub.status.busy":"2022-02-13T07:29:50.176781Z","iopub.execute_input":"2022-02-13T07:29:50.179747Z","iopub.status.idle":"2022-02-13T07:29:50.197130Z","shell.execute_reply.started":"2022-02-13T07:29:50.179691Z","shell.execute_reply":"2022-02-13T07:29:50.195678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict (model, tokenizer, clf, test_questions, test_passages)"],"metadata":{"id":"wdiywCCkzM4M","execution":{"iopub.status.busy":"2022-02-13T07:29:50.202591Z","iopub.execute_input":"2022-02-13T07:29:50.205911Z","iopub.status.idle":"2022-02-13T07:29:50.663691Z","shell.execute_reply.started":"2022-02-13T07:29:50.205854Z","shell.execute_reply":"2022-02-13T07:29:50.662757Z"},"trusted":true,"outputId":"420bd469-98fb-42d3-c341-b094a6c566de"},"execution_count":null,"outputs":[{"name":"stdout","text":"Q.:  is manic depression the same as bi polar\nYES\nQ.:  was whiskey galore based on a true story\nNO\nQ.:  are there plants on the international space station\nYES\nQ.:  does the hockey puck have to cross the line to be a goal\nNO\nQ.:  will there be a season 5 of shadowhunters\nNO\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Плоховатые ответы."],"metadata":{"id":"k0W7Iq4MzM4M"}},{"cell_type":"markdown","source":["Продолжим эксперименты в следующем ноутбуке."],"metadata":{"id":"Rx_8xwl3m_F5"}},{"cell_type":"code","source":[""],"metadata":{"id":"LjcG2MpVnCtz"},"execution_count":null,"outputs":[]}]}